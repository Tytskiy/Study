\documentclass[12pt,fleqn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb,amsmath,mathrsfs,amsthm}
\usepackage[russian]{babel}
\usepackage[pdftex]{graphicx}
\usepackage{multirow}
\usepackage{indentfirst}
\usepackage[colorlinks,linkcolor=blue(ryb),citecolor=blue(ryb), unicode]{hyperref}

\usepackage{xcolor}
\usepackage{sectsty}

\usepackage{amsmath}
\usepackage{systeme}
\definecolor{blue(ryb)}{rgb}{0.2, 0.2, 0.6}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


%\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
%\usepackage[all]{xy}

% Параметры страницы
\sectionfont{\color{blue(ryb)}}
\subsubsectionfont{\color{blue(ryb)}}
\subsectionfont{\color{blue(ryb)}}
\textheight=24cm % высота текста
\textwidth=16cm % ширина текста
\oddsidemargin=0pt % отступ от левого края
\topmargin=-2.5cm % отступ от верхнего края
\parindent=24pt % абзацный отступ
\parskip=0pt % интервал между абзацам
\tolerance=2000 % терпимость к "жидким" строкам
\flushbottom % выравнивание высоты страниц
%\def\baselinestretch{1.5}
\setcounter{secnumdepth}{0}
\renewcommand{\baselinestretch}{1.1}

\newcommand{\norm}{\mathop{\rm norm}\limits}
\newcommand{\real}{\mathbb{R}}

\newcommand{\ex}{\mathbb{E}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\intset}{\mathrm{int}}
\newcommand{\softmax}{\mathop{\rm softmax}\limits}
\newcommand{\lossfunc}{\mathcal{L}'}
\newcommand{\elbo}{\mathcal{L}}
\newcommand{\normal}[3]{\mathcal{N}(#1 | #2, #3)}
\newcommand{\dd}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\kl}[2]{\mathop{KL}(#1 \parallel #2)}
\newcommand{\nm}{\mathcal{N}}
\newcommand{\sle}{\; \Rightarrow \;}
\newcommand{\indpos}{\mathbf{I}_{d_k}^{+}[i, j]}
\newcommand{\indneg}{\mathbf{I}_{d_k}^{-}[i, j]}

\usepackage{pgfplots}

%my settings
\graphicspath{{../_figures/}}
\usepackage{wrapfig}
\usepackage[font=small]{caption}
\usepackage{multirow}

\title{Исследование работы линейных моделей}

\author{Тыцкий Владислав}
\date{Ноябрь 2020}

\begin{document}

\maketitle
\section{Градиент в логистической регрессии}
Пусть $X \in R^{N\times F}$ -- матрица объектов-признаков, $y \in \{-1,1\}^N$ -- метки соответствующих
объектов, $w \in R^F$ -- вектор весов, $x_i$ -- i-ый объект $ y_i$ -- метка класса i-ого
объекта,  T - стохастическая подвыборка($|T|$ и T будут обозначаться Т в зависимости от контекста).
Везде будем считать, что добавлен константый признак, но соответствующий вес не регуляризуется.

Дана задача оптимизации:
\begin{align}
    \notag & Q(X,y,w) = \mathcal{L}(X,y,w)+\frac{\lambda}{2}||w||_2^2 \rightarrow \min_{w} \\
    \notag & \mathcal{L}(X,y,w) = \frac{1}{T}\sum_{i=1}^{T}\log(1+\exp(-y_i\langle x_i, w\rangle), T \leq N
\end{align}
Для решения этой задачи с помощью градиентных методов необходимо знать градиент функционала $Q(X,y,w)$

\begin{align}
    \notag dQ = &d\mathcal{L} + \frac{\lambda}{2}d\langle w,w \rangle = d\mathcal{L} + \lambda\langle w, dw \rangle \\
    \notag d\mathcal{L} = &\frac{1}{T}\sum_{i=1}^{T}\frac{d(\exp(-y_i\langle x_i, w\rangle))}{1+\exp(-y_i\langle x_i, w\rangle)} =
    \frac{1}{T}\sum_{i=1}^{T}\frac{\exp(-y_i\langle x_i, w\rangle)d\langle-y_ix_i,w\rangle}{1+\exp(-y_i\langle x_i, w\rangle)} = \\
    \notag -&\frac{1}{T}\sum_{i=1}^{T}\frac{\langle y_ix_i,dw\rangle}{1+\exp(y_i\langle x_i, w\rangle)} 
\end{align}
Заметим, что $dQ = \langle \nabla Q,dw\rangle$. Окончательно получаем:
\begin{align}
    \notag \nabla Q(X,y,w) = \lambda w -\frac{1}{T}\sum_{i=1}^{T}\frac{y_ix_i}{1+\exp(y_i\langle x_i, w\rangle)} 
\end{align}
\subsection{Случай для K классов}
Пусть $X \in R^{N\times F}$ -- матрица объектов-признаков, $y \in K^N$ -- метки соответствующих
объектов, где $K=\{1\dots k\}$ -- множество классов, $w_i \in R^F$ -- вектор весов соответствующий k-ому классу, 
$x_i$ -- i-ый объект $ y_i$ -- метка класса i-ого
объекта соответственно, T - стохастическая подвыборка($|T|$ и T будут обозначаться 
Т в зависимости от контекста).

Дана задача оптимизации --- максимизация правдоподобия:
\begin{align}
    \notag & Q(X,y,w) = -\frac{1}{T}\sum_{i=1}^{T}\log\mathbb{P}(y_i|x_i)+
    \frac{\lambda}{2}\sum_{k=1}^{K}||w_k||_2^2 \rightarrow \min_{w_1\dots w_k} \\
    \notag & \mathbb{P}(y=c|x) = \frac{\exp{\langle w_c,x \rangle}}{\sum_{k=1}^{K}\exp{\langle w_k,x \rangle}}
\end{align}
Найдем градиент по $w_m$.
\begin{align}
    \notag & dQ(X,y,w) = d(-\frac{1}{T}\sum_{i=1}^{T}\log\mathbb{P}(y_i|x_i)) + d(\frac{\lambda}{2}\sum_{k=1}^{K}||w_k||_2^2) =
    -\frac{1}{T}d(\sum_{i=1}^{T}\log\mathbb{P}(y_i|x_i)) + \lambda(w_m,dw_m) \\
    \notag & d(\sum_{i=1}^{T}\log\mathbb{P}(y_i|x_i))_{w_m} = \sum_{i=1}^{T}d(\log\exp{\langle w_{y_i}, x_i \rangle})-
    \sum_{i=1}^{T}d(\log \sum_{k=1}^{K}\exp{\langle w_k, x_i \rangle}) = \\
    \notag =&\sum_{\substack{i:y_i \leftrightarrow w_m \\ i\in T}}\langle x_i,dw_m \rangle - 
    \sum_{i=1}^{T}\frac{\exp{\langle w_m,x_i\rangle}\langle x_i,dw_m \rangle}{\sum_{k=1}^{K}\exp{\langle w_k, x_i \rangle}}
\end{align}
Отсюда получаем: 
$$
\nabla Q_{w_m} =\lambda w_m + \frac{1}{T}\sum_{i=1}^{T}
\frac{\exp{\langle w_m,x_i\rangle}x_i}{\sum_{k=1}^{K}\exp{\langle w_k, x_i \rangle}}-
\frac{1}{T}\sum_{\substack{i:y_i \leftrightarrow w_m \\ i\in T}}x_i 
$$
\subsection{Эквивалентность бинарной логистической регрессии и мультиномиальной при K=2}
\noindent \textit{Доказательство}.

\noindent Пусть $w_{+}$ --- вектор весов соответствующий первому классу, а $w_{-}$ --- -1 классу.

\noindent Введем $w = w_{+}-w_{-}$.
\noindent Рассмотрим задачу мультиномиальной регрессии при K=2:

\begin{align}
    \notag & Q(X,y,w) = -\frac{1}{T}\sum_{i=1}^{T}\log
    \frac{\exp{\langle w_c,x \rangle}}{\sum_{k=1}^{K}\exp{\langle w_k,x \rangle}}=
     = - \frac{1}{T}\sum_{\substack{i:y_i=w_{+} \\ i\in T}}
    \log\frac{\exp{\langle w_{+}, x_i \rangle}}{\exp{\langle w_{+}, x_i \rangle}+\exp{\langle w_{-}, x_i \rangle}}\\
    \notag - &\frac{1}{T}\sum_{\substack{i:y_i=w_{-} \\ i\in T}}
    \log\frac{\exp{\langle w_{-}, x_i \rangle}}{\exp{\langle w_{+}, x_i \rangle}+\exp{\langle w_{-}, x_i \rangle}} = 
     - \frac{1}{T}\sum_{\substack{i:y_i=w_{+} \\ i\in T}}\log\frac{1}{1+\exp{\langle -w, x_i \rangle}}- \\
    \notag- &\frac{1}{T}\sum_{\substack{i:y_i=w_{-} \\ i\in T}}\log\frac{1}{\exp{\langle w, x_i \rangle} + 1} = 
    = \frac{1}{T}\sum_{i=1}^{T}\log(1+\exp(-y_i\langle w,x_i \rangle))
\end{align}
То есть функции потерь для бинарной логрегрессии и мультиномиальной регрессии 
при K=2 эквиваленты.

\noindent\textit{ч.т.д}
\section{Задание №1}
\noindent Предобработка документов делается с помощью модуля re и метода apply из pandas.
\begin{lstlisting}[language=Python, caption=Clear documents]
    X_train_df.apply(lambda x: re.sub("[^a-zA-z0-9]", " ", x.lower()))
    X_test_df.apply(lambda x: re.sub("[^a-zA-z0-9]", " ", x.lower()))
\end{lstlisting}
\section{Задание №2}
\noindent Использование CountVectorizer для представления слов с помощью bag of words.

\begin{lstlisting}[language=Python, caption=Vectorizer]
vectorizer = CountVectorizer(lowercase=True, min_df=50)
X_train_v = vectorizer.fit_transform(X_train["comment_text"])
X_test_v = vectorizer.transform(X_test["comment_text"])
\end{lstlisting}

\noindent Min\_df = 50 имеет под собой основание. ''Оскорбительные'' слова в данном датасете обычно
встречаются чаще 100 раз. Листинг ниже (\ref{code:count_bad}) демонстрирует код для подсчета.

\begin{lstlisting}[language=Python, caption=Count bad words, label=code:count_bad]
text = X_train["comment_text"]
count = 0
count_bad = 0
for i in range(text.size):
    if text[i].find("very very bad word") != -1:
        count += 1
        if X_train["is_toxic"][i]:
            count_bad += 1
\end{lstlisting}

\section{Задание №3, №4, №5}
Исследуем как ведет себя метод (стохастического) градиентного спуска. 
Я посчитал, что 3, 4, 5 задания можно совместить в одно большое задание --- легче прослеживается
логика повествования.
\footnote{Все графики строились на весьма урезанной по количеству признаков выборке (2300).
 Это сделано для того, чтобы вычислительной мощности компьютера Тыцкого.В.И. хватило построить
 их за разумное время.}

\subsection{Начальная инициализация}
Интересно взглянуть как влияет начальная инициализация весов на функцию потерь (Таблица \ref{pic:weights})
\footnote{Я так и не понял как поменять тип caption с Таблицы на Рисунок}

\newpage

\begin{table}[htb]
    \centering
    \tabcolsep = -10pt
    \begin{tabular}{cc}
        \includegraphics[width=9cm]{/task_3/weights.pdf}  & \includegraphics[width=9cm]{/task_4/weights.pdf} \\
        GD & SGD
    \end{tabular}
    \caption{Зависимость функции потерь от начальной инициализации и итераций}
    \label{pic:weights}
\end{table}

Можно заметить, что вне зависимости от начальной инициализации спустя небольшое
количество итераций(эпох) функция потерь становится примерно одинаковой.

\textbf{В других экспериментах будем использовать нулевой вектор в качестве начальной инициализации}
\subsection{Параметры задающие скорость обучения}
В экспериментах используется линейный классификатор, который вычисляет новый вес $w^{i+1}$ 
по формуле:

$$
    w^{i+1} = w^i - \eta  \nabla Q, \:
    \eta = \frac{\alpha}{i^\beta}
$$
где Q - градиент функции потерь.

\noindent Необходимо понять как влияют гиперпараметры $\alpha$ и $\beta$ на алгоритм.
\noindent В Таблице \ref{pic:GD_alpha_beta} представлены 
графики зависимости функции потерь от параметров $\alpha$ и $\beta$.

\begin{table}[htb]
    \centering
    \tabcolsep = -10pt
    \begin{tabular}{ccc}
        \includegraphics[width=6cm]{/task_3/sub_1/fig_a_0.1_b_1.pdf}  & 
        \includegraphics[width=6cm]{/task_3/sub_1/fig_a_1.0_b_1.pdf} &
         \includegraphics[width=6cm]{/task_3/sub_1/fig_a_10.0_b_1.pdf} \\
         $\alpha = 0.1$ & $\alpha = 1.0$ & $\alpha = 10.0$
    \end{tabular}
    \caption{Зависимость функции потерь от alpha и beta для GD}
    \label{pic:GD_alpha_beta}
\end{table}

\newpage

Заметим как быстро алгоритм останавливается при $\beta>0$. Если и имеет смысл 
использовать ненулевые $\beta$, то только для больших значений параметра $\alpha$.
В то же время при сильно больших $\alpha$ и $\beta=0$ градиентный спуск может не
спуститься в точку экстремума, что плохо сказывается на качестве модели.

\textbf{В дальнейших экспериментах будем брать $\alpha\thickapprox 0.5$ и $\beta=0$.}

Для стохастического градиентного спуска картина такая же
(Таблица \ref{pic:SGD_alpha_beta}) за исключением того, что при 
больших $\alpha$ поведение еще более непредсказуемо. 

\begin{table}[htb]
    \centering
    \tabcolsep = -10pt
    \begin{tabular}{ccc}
        \includegraphics[width=6cm]{/task_4/sub_1/fig_a_0.1_b_1.pdf}  & 
        \includegraphics[width=6cm]{/task_4/sub_1/fig_a_1.0_b_1.pdf} &
         \includegraphics[width=6cm]{/task_4/sub_1/fig_a_10.0_b_1.pdf} \\
         $\alpha = 0.1$ & $\alpha = 1.0$ & $\alpha = 10.0$
    \end{tabular}
    \caption{Зависимость функции потерь от alpha и beta для SGD}
    \label{pic:SGD_alpha_beta}
\end{table}

\subsection{Сравнение GD и SGD в скорости}
На предыдущих графиках(Таблица \ref{pic:GD_alpha_beta}, Таблица \ref{pic:SGD_alpha_beta})
можно пронаблюдать скорость обучения GD и SGD классификатора. GD делает более ''точныe''
шаги градиентного спуска, но скорость выполнения этого шага довольно медленная. 
Хоть SGD менее точен, но из-за того,
что он делает больше шагов градиентного спуска за эпоху, он быстрее сходится к локальному экстремуму. 
Заметим, что выбор в пользу SGD сделан конкретно для данного датасета. Для других задач 
поведение GD и SGD может быть совсем иным.

\textbf{В дальнейших экспериментах будем использовать SGD классификатор.}

\subsection{Время на одну итерацию (эпоху)}
Важно оценить время работы классификатора в зависимости от итераций (эпох), чтобы подобрать
оптимальное по соотношению качество скорость итераций (эпох). Таблица \ref{pic:time_iter}.
Одна итерация (эпоха) делается чуть меньше, чем за секунду.

\begin{table}[htb]

    \centering
    \tabcolsep = -10pt
    \begin{tabular}{cc}
        \includegraphics[width=6cm]{/task_3/sub_1/fig_a_0.1_b_1.pdf}  & 
        \includegraphics[width=6cm]{/task_3/sub_2/fig_a_0.1_b_1.pdf} \\
        Time & iterations GD \\
         \includegraphics[width=6cm]{/task_4/sub_1/fig_a_0.1_b_1.pdf} &
         \includegraphics[width=6cm]{/task_4/sub_2/fig_a_0.1_b_1.pdf} \\
         Time & epoch SGD
    \end{tabular}
    \caption{Зависимость между временем и итерацией (эпохой)}
    \label{pic:time_iter}
\end{table}

\newpage
\subsection{Размер батча для SGD}
\begin{wrapfigure}[13]{r}{0.5\textwidth}
    \includegraphics[width=8cm]{/task_4/fig_batch.pdf} 
    \captionsetup{font={scriptsize,it}}
    \caption{Зависимость функции потерь от размера батча}
    \label{pic:batch}
\end{wrapfigure}

В случае выбора SGD в качестве основного алгоритма необходимо понять какой размер батча (подвыборки)
оптимален. Важна и скорость работы, и точность шагов градиентного спуска. Справа представлен
график Рис.\ref{pic:batch}.


Можно заметить, что уже при размерах батча 2000 скорость сходимости и
точность шагов градиентного спуска приемлемы --- не возникает больших скачков, как у размера 500
и скорость гораздо выше, чем для размера 10000 или всей выборки.

\textbf{В будущих экспериментах будет использоваться размер батча 2000-3000.}

\subsection{Качество алгоритма}
Уменьшение функции потерь --- не самое важное для нас. Необходимо взглянуть как меняется мера
качества в идеале на отложенной выборке.\footnote{Не стал делать отложенную выборку и замерил на обучающей} 
Ниже представлены графики Таблица \ref{pic:accuracy} для SGD классификатора(batch\_size=2000,
l2\_coef=0.1)

\begin{table}[htb]
    \centering
    \tabcolsep = -10pt
    \begin{tabular}{ccc}
        \includegraphics[width=6cm]{/task_4/sub_3/fig_a_0.1_b_1.pdf}  & 
        \includegraphics[width=6cm]{/task_4/sub_3/fig_a_1.0_b_1.pdf} &
         \includegraphics[width=6cm]{/task_4/sub_3/fig_a_10.0_b_1.pdf} \\
         $\alpha = 0.1$ & $\alpha = 1.0$ & $\alpha = 10.0$
    \end{tabular}
    \caption{Зависимость функции потерь от alpha и beta для GD}
    \label{pic:accuracy}
\end{table}

Лучшее качество на данном этапе экспериментов для тестовой выборки:
\textbf{roc-auc-- 0.928, accuracy-- 0.860}

Обучение проводилось на выборке, векторизованной Bag of words с $min\_df=50$,
с помощью SGD классификатора с параметрами:\newline
$batch\_size=2000, \alpha = 0.5, \beta = 0, max\_iter=1500, l2\_coef=0.01, tolerance=1e-7,
fit\_intercept=True$.
\section{Задание №6}
При обработке текстов возникает проблема --- одно и то же слово может присутствовать
в коллекции в разных формах (например play, played). Все слова стоит привести в начальную форму --
это может существенно уменьшить признаковое пространство и улучшить качество модели. 
Возьмем за основу алгритм лемматизации слов. Ниже представлен Листинг \ref{code:lemm}, 
демонстрирующий применение алгоритма лемматизации с помощью библиотеки nltk.

\begin{lstlisting}[language=Python, caption=Lemmatizer,label=code:lemm]
# https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/
def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)


lem= WordNetLemmatizer()

X_train.apply(lambda x: " ".join([lem.lemmatize(w, get_wordnet_pos(w)) 
        for w in nltk.word_tokenize(x)]))
X_test.apply(lambda x: " ".join([lemm.lemmatize(w, get_wordnet_pos(w)) 
        for w in nltk.word_tokenize(x)]))
\end{lstlisting}

\noindentТакже перед лемматизацией удалим стоп-слова.

Размерность признакового пространства снизилась с 3930 до 3090. На скорость работы
уменьшение размерности несильно повлияло\footnote{в виду того, что и так размерность не самая большая}.
Accuracy и roc-auc на тестовой выборке существенно возросло:
\textbf{roc-auc -- 0.935, accuracy -- 0.865}

Обучение проводилось на лемматизированной выборке, векторизованной Bag of words с 
$min\_df=50, stop\_words=english$,
с помощью SGD классификатора с параметрами:\newline
$batch\_size=2000, \alpha = 0.8, \beta = 0, max\_iter=1800, l2\_coef=0.01, tolerance=1e-7,
fit\_intercept=False$.


\section{Задание №7}
Посмотрим как влияет предобработка данных на качество, а конкретно параметры min\_df, max\_df.
В Таблице \ref{table:min_max_df} представлены зависимости времени, размерности,
 качества (BoW, TF-IDF) от min\_df, max\_df.
\begin{table}[htb]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    \multicolumn{2}{|l|}{} & shape & time s. & \begin{tabular}[c]{@{}l@{}}accuracy\\ BoW\end{tabular} & \begin{tabular}[c]{@{}l@{}}accuracy\\ TFIDF\end{tabular} \\ \hline
    \multirow{3}{*}{max\_df = max} & min\_df = 10    & 11000 & 23    & 0.851 & 0.741 \\ \cline{2-6} 
                                  & min\_df = 100   & 2080  & 7.5   & 0.850 & 0.831 \\ \cline{2-6} 
                                  & min\_df  = 1000 & 150   & 2.0   & 0.784 & 0.784 \\ \hline
    \multirow{3}{*}{min\_df = 50} & max\_df  = 300  & 2957  & 8.27  & 0.723 & 0.698 \\ \cline{2-6} 
                                  & max\_df = 800  & 3452  & 9.59  & 0.801 & 0.725 \\ \cline{2-6} 
                                  & max\_df = max   & 3660  & 10.79 & 0.848 & 0.766 \\ \hline
    \end{tabular}
    \caption{Зависимость некоторых величин от min\_df, max\_df и типов векторизации}
    \label{table:min_max_df}
\end{table}

Размерность пространства сильно увеличивается, если min\_df близко к нулю, но качество
в свою очередь несильно меняется. Как было упомянутно в начале исследования, обычно  ''плохие''
слова встречаются чаще, чем 100 раз на коллекцию, поэтому уменьшение min\_df приводит только 
к увеличению размерности и времени работы.

Max\_df неразумно ставить не максимальным т.к. мы можем потерять
важную информацию, а самые распространенные слова (стоп-слова) английского языка мы предварительно
убрали.

Интересно, что качество после обработки с помощью TF-IDF стабильно хуже, чем BoW. \textbf{Лучший алгоритм
показал roc-auc -- 0.935, accuracy -- 0.853 на тестовой выборке.(TF-IDF)}

\section{Задание №8}
Самый лучший алгортим показал \textbf{auc-roc -- 0.937, accuracy -- 0.876 на тестовой выборке}.
Использовался лемматизированный BoW с 2-gramm, стоп-словами, и mid\_df=50.

Параметры алгоритма: batch\_size=2000, step\_alpha=0.4, step\_beta=0,
tolerance=1e-7, max\_iter=3000, l2\_coef=0.0001, fit\_intercept=True

Проанализируем ошибки алгоритма:

\subsubsection{False positive}
\begin{enumerate}
    \item "i will burn you to hell if you revoke my talk page access"
    \item "what the hell justin"
    \item "black mamba it be ponious snake of the word
    and but it not kill many people but king cobra kill many people in india"
\end{enumerate}

Для некоторых из FP объектов мог ошибиться даже человек (1),(2)\footnote{По крайней мере я}, 
в некоторых документах использовались слова характерные для токсичных комментариев, но на деле 
это было не так (3).


\subsubsection{False negative}
Примеры: 
\begin{enumerate}
    \item "hey shithead stop vandilizing article"
    \item "i think you should delete the whole fuckin wikipedia"
    \item "shut up please this consensus be over"
    \item "if ya not still fu k u"
    \item "x box 360 sukcs big bumm and like it up the as"
\end{enumerate}

Все ошибки можно разделить на два типа: алгоритм объективно ошибся (1),(2),(3) или в документе
были допущены ошибки (намерено или случайно), которые не позволили правильно 
классифицировать документ (4),(5).

\subsubsection{Вывод}
Алгоритм хорошо научился распознавать токсичные комметарии, в которых нет намеренных ошибок 
в ''плохих'' словах, но сложности возникают, когда слова специально меняют -- добавляют пробелы, меняют
буквы местами, заменяют буквы другими символами. Также алгоритм плохо понимает контекст и ложно относит 
к токсичным комментарии, которые были на определенную тему(например комментарий про кобр).

Возникает гипотеза, что в разметке могли быть допушены ошибки, потому что некоторые из комментариев
явно относятся к токсичным, но в датасете отмечены как не токсичные.

\end{document}